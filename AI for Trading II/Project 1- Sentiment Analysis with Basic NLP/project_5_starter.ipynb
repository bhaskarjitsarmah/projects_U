{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Sentiment Analysis with Basic NLP\n",
    "## Instructions\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment. After implementing the function, run the cell to test it against the unit tests we've provided. For each problem, we provide one or more unit tests from our `project_tests` package. These unit tests won't tell you if your answer is correct, but will warn you of any major errors. Your code will be checked for the correct solution when you submit it to Udacity.\n",
    "\n",
    "## Packages\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "The other packages that we're importing are `project_helper` and `project_tests`. These are custom packages built to help you solve the problems.  The `project_helper` module contains utility functions and graph functions. The `project_tests` contains the unit tests for all the problems.\n",
    "\n",
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.3.0 (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 370kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.13.3 (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/57/a7/e3e6bd9d595125e1abbe162e323fd2d06f6f6683185294b79cd2cdb190d5/numpy-1.13.3-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.0MB 31kB/s  eta 0:00:01   16% |█████▍                          | 2.9MB 25.2MB/s eta 0:00:01    23% |███████▋                        | 4.0MB 21.1MB/s eta 0:00:01    30% |█████████▊                      | 5.2MB 25.4MB/s eta 0:00:01    51% |████████████████▋               | 8.8MB 18.1MB/s eta 0:00:01    65% |█████████████████████           | 11.1MB 24.9MB/s eta 0:00:01    79% |█████████████████████████▌      | 13.5MB 25.0MB/s eta 0:00:01    86% |███████████████████████████▊    | 14.7MB 25.5MB/s eta 0:00:01    92% |█████████████████████████████▊  | 15.7MB 23.5MB/s eta 0:00:01    98% |███████████████████████████████▌| 16.7MB 10.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ratelimit==2.2.0 (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/b5/73/956d739706da2f74891ba46391381ce7e680dce27cce90df7c706512d5bf/ratelimit-2.2.0.tar.gz\n",
      "Requirement already satisfied: requests==2.18.4 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4))\n",
      "Requirement already satisfied: scikit-learn==0.19.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5))\n",
      "Requirement already satisfied: six==1.11.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6))\n",
      "Collecting tqdm==4.19.5 (from -r requirements.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/71/3c/341b4fa23cb3abc335207dba057c790f3bb329f6757e1fcd5d347bcf8308/tqdm-4.19.5-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 4.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 4))\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 4))\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 4))\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 4))\n",
      "Building wheels for collected packages: nltk, ratelimit\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
      "  Running setup.py bdist_wheel for ratelimit ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a6/2a/13/3c6e42757ca0b6873a60e0697d30f7dd9d521a52874c44f201\n",
      "Successfully built nltk ratelimit\n",
      "Installing collected packages: nltk, numpy, ratelimit, tqdm\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "  Found existing installation: tqdm 4.11.2\n",
      "    Uninstalling tqdm-4.11.2:\n",
      "      Successfully uninstalled tqdm-4.11.2\n",
      "Successfully installed nltk-3.3 numpy-1.13.3 ratelimit-2.2.0 tqdm-4.19.5\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "import project_helper\n",
    "import project_tests\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLP Corpora\n",
    "You'll need two corpora to run this project: the stopwords corpus for removing stopwords and wordnet for lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 10ks\n",
    "We'll be running NLP anlalysis on 10-k documents. To do that, we first need to download the documents. For this project, we'll download 10-ks for Apple and Amazon. To lookup documents for these companies, we'll use their CIK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_lookup = {'AMZN': '0000320193', 'AAPL': '0001018724'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of 10-ks\n",
    "Let's pull a list of filled 10-ks from the SEC for each company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_sec_data(cik, doc_type, start=0, count=10):\n",
    "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
    "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
    "        .format(cik, doc_type, start, count)\n",
    "    page = project_helper.call_sec(rss_url)\n",
    "    feed = BeautifulSoup(page.text.encode('ascii'), 'xml').feed\n",
    "    entries = [\n",
    "        (\n",
    "            entry.content.find('filing-href').getText(),\n",
    "            entry.content.find('filing-type').getText(),\n",
    "            entry.content.find('filing-date').getText())\n",
    "        for entry in feed.find_all('entry', recursive=False)]\n",
    "\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull the list using the `get_sec_data` function, then display some of the results. For displaying some of the data, we'll use Amazon as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('http://www.sec.gov/Archives/edgar/data/320193/000032019318000145/0000320193-18-000145-index.htm',\n",
      "  '10-K',\n",
      "  '2018-11-05'),\n",
      " ('http://www.sec.gov/Archives/edgar/data/320193/000032019317000070/0000320193-17-000070-index.htm',\n",
      "  '10-K',\n",
      "  '2017-11-03'),\n",
      " ('http://www.sec.gov/Archives/edgar/data/320193/000162828016020309/0001628280-16-020309-index.htm',\n",
      "  '10-K',\n",
      "  '2016-10-26'),\n",
      " ('http://www.sec.gov/Archives/edgar/data/320193/000119312515356351/0001193125-15-356351-index.htm',\n",
      "  '10-K',\n",
      "  '2015-10-28'),\n",
      " ('http://www.sec.gov/Archives/edgar/data/320193/000119312514383437/0001193125-14-383437-index.htm',\n",
      "  '10-K',\n",
      "  '2014-10-27')]\n"
     ]
    }
   ],
   "source": [
    "example_ticker = 'AMZN'\n",
    "sec_data = {}\n",
    "\n",
    "for ticker, cik in cik_lookup.items():\n",
    "    sec_data[ticker] = get_sec_data(cik, '10-K')\n",
    "\n",
    "pprint.pprint(sec_data[example_ticker][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download 10-ks\n",
    "As you see, this is a list of urls. These urls point to a file that contains meta data related to each filling. Since we don't care about the meta data, we'll pull the filling by replacing the url with the filling url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings: 100%|██████████| 10/10 [00:04<00:00,  2.26filling/s]\n",
      "Downloading AAPL Fillings: 100%|██████████| 10/10 [00:02<00:00,  3.34filling/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Document:\n",
      "\n",
      "<SEC-DOCUMENT>0000320193-18-000145.txt : 20181105\n",
      "<SEC-HEADER>0000320193-18-000145.hdr.sgml : 20181105\n",
      "<ACCEPTANCE-DATETIME>20181105080140\n",
      "ACCESSION NUMBER:\t\t0000320193-18-000145\n",
      "CONFORMED SUBMISSION TYPE:\t10-K\n",
      "PUBLIC DOCUMENT COUNT:\t\t88\n",
      "CONFORMED PERIOD OF REPORT:\t20180929\n",
      "FILED AS OF DATE:\t\t20181105\n",
      "DATE AS OF CHANGE:\t\t20181105\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\t\n",
      "\t\tCOMPANY CONFORMED NAME:\t\t\tAPPLE INC\n",
      "\t\tCENTRAL INDEX KEY:\t\t\t0000320193\n",
      "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tELECTRONIC COMPUTERS [3571]\n",
      "\t\tIRS NUMBER:\t\t\t\t942404110\n",
      "\t\tSTATE OF INCORPORATION:\t\t\tCA\n",
      "\t\tFISCAL YEAR END:\t\t\t0930\n",
      "\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:\t\t10-K\n",
      "\t\tSEC ACT:\t\t1934 Act\n",
      "\t\tSEC FILE NUMBER:\t001-36743\n",
      "\t\tFILM NUMBER:\t\t181158788\n",
      "\n",
      "\tBUSINESS ADDRESS:\t\n",
      "\t\tSTREET 1:\t\tONE APPLE PARK WAY\n",
      "\t\tCITY:\t\t\tCUPERTINO\n",
      "\t\tSTATE:\t\t\tCA\n",
      "\t\tZIP:\t\t\t95014\n",
      "\t\tBUSINESS PHONE:\t\t(408) 996-1010\n",
      "\n",
      "\tMAIL ADDRESS:\t\n",
      "\t\tSTREET 1:\t\tONE APPLE PARK WAY\n",
      "\t\tCITY:\t\t\tCUPERTINO\n",
      "\t\tSTATE:\t\t\tCA\n",
      "\t\tZIP:\t\t\t95014\n",
      "\n",
      "\tFORMER COMPANY:\t\n",
      "\t\tFORMER CONFORMED NAME:\tAPPLE COMPUTER INC\n",
      "\t\tDATE OF NA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_fillings_by_ticker = {}\n",
    "for ticker, data in sec_data.items():\n",
    "    raw_fillings_by_ticker[ticker] = {}\n",
    "    for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n",
    "        if (file_type == '10-K'):\n",
    "            file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n",
    "            \n",
    "            raw_fillings_by_ticker[ticker][file_date] = project_helper.call_sec(file_url).text\n",
    "\n",
    "\n",
    "print('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Documents\n",
    "With theses fillings downloaded, we want to break them into their associated documents. These documents are sectioned off in the fillings with the tags `<DOCUMENT>` for the start of each document and `</DOCUMENT>` for the end of each document. There's no overlap with these documents, so each `</DOCUMENT>` tag should come after the `<DOCUMENT>` with no `<DOCUMENT>` tag inbetween.\n",
    "\n",
    "Implement `get_documents` to return a list of these documents from a filling. Make sure not to include the tag in the returned document text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_documents(text):\n",
    "    \"\"\"\n",
    "    Extract the documents from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text with the document strings inside\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    extracted_docs : list of str\n",
    "        The document strings found in `text`\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_get_documents(get_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `get_documents` function implemented, let's extract all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filling_documents_by_ticker = {}\n",
    "for ticker, raw_fillings in raw_fillings_by_ticker.items():\n",
    "    filling_documents_by_ticker[ticker] = {}\n",
    "    for file_date, filling in raw_fillings.items():\n",
    "        filling_documents_by_ticker[ticker][file_date] = get_documents(filling)\n",
    "\n",
    "\n",
    "print('\\n\\n'.join([\n",
    "    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
    "    for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n",
    "    for doc_i, doc in enumerate(docs)][:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Document Types\n",
    "Not that we have all the documents, we want to find the 10-k form in this 10-k filling. Implement the `get_document_type` function to return the type of document given. The document type is located on a line with the `<TYPE>` tag. For example, a form of type \"TEST\" would have the line `<TYPE>TEST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_type(doc):\n",
    "    \"\"\"\n",
    "    Return the document type lowercased\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : str\n",
    "        The document string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    doc_type : str\n",
    "        The document type lowercased\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_get_document_type(get_document_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `get_document_type` function, we'll filter out all non 10-k documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_ks_by_ticker = {}\n",
    "for ticker, filling_documents in filling_documents_by_ticker.items():\n",
    "    ten_ks_by_ticker[ticker] = []\n",
    "    for file_date, documents in filling_documents.items():\n",
    "        for document in documents:\n",
    "            if get_document_type(document) == '10-k':\n",
    "                ten_ks_by_ticker[ticker].append({\n",
    "                    'cik': cik_lookup[ticker],\n",
    "                    'file': document,\n",
    "                    'file_date': file_date})\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Items\n",
    "Each 10-k form is broken into items. For this project, we want item 1a, 7, and 7a. We'll use the function `find_items` to extract the items from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_items(ten_k):\n",
    "    fin_pattern=re.compile(r'(>Item(\\s|&#160;|&nbsp;|)[01]{0,1}[0-9][AB]{0,1}\\.{0,1})|(ITEM\\s[01]{0,1}[0-9][AB]{0,1})')\n",
    "    fin_match=re.finditer(fin_pattern,(ten_k))\n",
    "    \n",
    "    test_df=pd.DataFrame([(x.group(),x.start(),x.end()) for x in (fin_match)])\n",
    "    test_df.columns=['item','start','end']\n",
    "    test_df['item']=test_df.item.str.lower()\n",
    "    test_df.replace('&#160;',' ',regex=True,inplace=True)\n",
    "    test_df.replace('&nbsp;',' ',regex=True,inplace=True)\n",
    "    test_df.replace('\\.','',regex=True,inplace=True)\n",
    "    test_df.replace({'item': r'^>'}, {'item': ' '}, regex=True,inplace=True)\n",
    "    \n",
    "    pos_dat=test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'],keep='last')\n",
    "    pos_dat.replace(' ','',regex=True,inplace=True)\n",
    "    pos_dat.loc[:,('start_next')]=(pos_dat.start.shift(-1))\n",
    "    pos_dat.start_next.replace(np.nan,len(ten_k),inplace=True)\n",
    "    pos_dat.loc[:,('end')]=pos_dat.start_next.astype('int')\n",
    "    pos_dat.drop(['start_next'],axis=1,inplace=True)\n",
    "    fin_dat=pos_dat.set_index('item').T.to_dict('list')\n",
    "    \n",
    "    item1a=ten_k[fin_dat['item1a'][0]:fin_dat['item1a'][1]]\n",
    "    item7=ten_k[fin_dat['item7'][0]:fin_dat['item7'][1]]\n",
    "    item7a=ten_k[fin_dat['item7a'][0]:fin_dat['item7a'][1]]\n",
    "    \n",
    "    return item1a, item7, item7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in ten_ks:\n",
    "        item1a, item7, item7a = find_items(ten_k['file'])\n",
    "        ten_k['item1a_raw'] = item1a\n",
    "        ten_k['item7_raw'] = item7\n",
    "        ten_k['item7a_raw'] = item7a\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['item1a_raw', 'item7_raw', 'item7a_raw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "### Clean Up\n",
    "As you can see, the text for the items is very messy. To clean this up, we'll remove the html and lowercase all the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_html_tags(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `clean_text` function, we'll clean up all the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in ten_ks:\n",
    "        ten_k['item1a'] = clean_text(ten_k['item1a_raw'])\n",
    "        ten_k['item7'] = clean_text(ten_k['item7_raw'])\n",
    "        ten_k['item7a'] = clean_text(ten_k['item7a_raw'])\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['item1a', 'item7', 'item7a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize\n",
    "With the text cleaned up, it's time to distill the verbs down. Implement the `lemmatize_words` function to lemmitize verbs in the list of words provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"\n",
    "    Lemmatize words \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words : list of str\n",
    "        List of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lemmatized_words : list of str\n",
    "        List of lemmatized words\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_lemmatize_words(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `lemmatize_words` function implemented, let's lemmatize all the item data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pattern = re.compile('\\w+')\n",
    "\n",
    "\n",
    "for _, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in ten_ks:\n",
    "        ten_k['lemma_item1a'] = lemmatize_words(word_pattern.findall(ten_k['item1a']))\n",
    "        ten_k['lemma_item7'] = lemmatize_words(word_pattern.findall(ten_k['item7']))  \n",
    "        ten_k['lemma_item7a'] = lemmatize_words(word_pattern.findall(ten_k['item7a']))\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['lemma_item1a', 'lemma_item7', 'lemma_item7a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on 10ks\n",
    "## Bag of Words\n",
    "Let's extract some features from the text data using bag of words. Implement the `get_bag_of_words` using sklearn's `CountVectorizer` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_bag_of_words(docs):\n",
    "    \"\"\"\n",
    "    Generate a bag of words from documents\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs : list of str\n",
    "        List of documents used to generate bag of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bag_of_words : 2-d Numpy Ndarray\n",
    "        Bag of words from the documents\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_get_bag_of_words(get_bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to generate the bag of words for each item corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_ten_ks = {}\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    bag_of_words_ten_ks[ticker] = {\n",
    "        'item1a': get_bag_of_words([' '.join(ten_k['lemma_item1a']) for ten_k in ten_ks]),\n",
    "        'item7': get_bag_of_words([' '.join(ten_k['lemma_item7']) for ten_k in ten_ks]),\n",
    "        'item7a': get_bag_of_words([' '.join(ten_k['lemma_item7a']) for ten_k in ten_ks])}\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data([bag_of_words_ten_ks[example_ticker]], ['item1a', 'item7', 'item7a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity\n",
    "Using the bag of words, let's calculate the jaccard similarty and plot it over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "\n",
    "def get_jaccard_similarity(docs):\n",
    "    similarites = []\n",
    "    for doc_i in range(len(docs) - 1):\n",
    "        similarites.append(jaccard_similarity_score(docs[doc_i], docs[doc_i + 1]))\n",
    "    \n",
    "    return similarites\n",
    "\n",
    "\n",
    "# Get dates for the universe\n",
    "file_dates = {\n",
    "    ticker: [ten_k['file_date'] for ten_k in ten_ks]\n",
    "    for ticker, ten_ks in ten_ks_by_ticker.items()}  \n",
    "\n",
    "item_1a_jaccard_similarities = get_jaccard_similarity(bag_of_words_ten_ks[example_ticker]['item1a'])\n",
    "item_7_jaccard_similarities = get_jaccard_similarity(bag_of_words_ten_ks[example_ticker]['item7'])\n",
    "item_7a_jaccard_similarities = get_jaccard_similarity(bag_of_words_ten_ks[example_ticker]['item7a'])\n",
    "\n",
    "\n",
    "project_helper.plot_similarities(\n",
    "    [item_1a_jaccard_similarities, item_7_jaccard_similarities, item_7a_jaccard_similarities],\n",
    "    file_dates[example_ticker][1:],\n",
    "    'Jaccard Similarities',\n",
    "    ['item 1a', 'item 7', 'item 7a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency\n",
    "Just like we did with the bag of words, lets create TFIDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def word_freq(docs):\n",
    "    return TfidfVectorizer().fit_transform(docs).toarray()\n",
    "\n",
    "\n",
    "word_freq_ten_ks = {}\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    word_freq_ten_ks[ticker] = {\n",
    "        'item1a': word_freq([' '.join(ten_k['lemma_item1a']) for ten_k in ten_ks]),\n",
    "        'item7': word_freq([' '.join(ten_k['lemma_item7']) for ten_k in ten_ks]),\n",
    "        'item7a': word_freq([' '.join(ten_k['lemma_item7a']) for ten_k in ten_ks])}\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data([word_freq_ten_ks[example_ticker]], ['item1a', 'item7', 'item7a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "Using the TFIDF values, we'll calculate the cosine similarity and plot it over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def get_cosine_similarity(docs):\n",
    "    return np.diag(cosine_similarity(docs, docs), k=1)\n",
    "\n",
    "\n",
    "item_1a_cosine_similarities = get_cosine_similarity(word_freq_ten_ks[example_ticker]['item1a'])\n",
    "item_7_cosine_similarities = get_cosine_similarity(word_freq_ten_ks[example_ticker]['item7'])\n",
    "item_7a_cosine_similarities = get_cosine_similarity(word_freq_ten_ks[example_ticker]['item7a'])\n",
    "\n",
    "\n",
    "project_helper.plot_similarities(\n",
    "    [item_1a_cosine_similarities, item_7_cosine_similarities, item_7a_cosine_similarities],\n",
    "    file_dates[example_ticker][1:],\n",
    "    'Cosine Similarities',\n",
    "    ['item 1a', 'item 7', 'item 7a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You've sucessfully done dentiment analysis on 10-ks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
